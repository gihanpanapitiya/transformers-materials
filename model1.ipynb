{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a9fedc",
   "metadata": {},
   "source": [
    "This implementation of the transformer in this model is based on the tutorial given at https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caac5121-727f-4c35-b730-f36b8939abc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "/Users/gihan/.env/p11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gihan/.env/p11/lib/python3.11/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'MXMNet' from 'deepchem.models.torch_models' (/Users/gihan/.env/p11/lib/python3.11/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import os\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "from deepchem.feat import HuggingFaceFeaturizer\n",
    "from transformers import RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc5233a-2c83-4d54-a092-1aec56819f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implementations you see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        if mask is not None:\n",
    "            # mask = expand_mask(mask)\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2).to(torch.bool)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, seq_length, heads]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits = attn_logits / math.sqrt(d_k)\n",
    "\n",
    "\n",
    "        attn_logits=attn_logits.masked_fill(mask, float('-inf') )\n",
    "        \n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        \n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "\n",
    "\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "    \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455a128b-76f7-43be-9d2e-9f347686f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, dropout=0.0, input_dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.model_dim = model_dim\n",
    "        self.input_dropout = input_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        self.input_net = torch.nn.Embedding(input_dim, self.model_dim, 0)\n",
    "        \n",
    "        # self.input_net = nn.Sequential(\n",
    "        #     nn.Dropout(self.input_dropout),\n",
    "        #     nn.Linear(self.input_dim, self.model_dim)\n",
    "        # )\n",
    "        # Positional encoding for sequences\n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.model_dim)\n",
    "        # Transformer\n",
    "        self.transformer = TransformerEncoder(num_layers=self.num_layers,\n",
    "                                              input_dim=self.model_dim,\n",
    "                                              dim_feedforward=2*self.model_dim,\n",
    "                                              num_heads=self.num_heads,\n",
    "                                              dropout=self.dropout)\n",
    "        # Output classifier per sequence lement\n",
    "\n",
    "\n",
    "        self.dense = nn.Linear(self.model_dim, 128)\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "        self.out_proj = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # self.output_net = nn.Sequential(\n",
    "        #     nn.Linear(self.model_dim, self.model_dim),\n",
    "        #     nn.LayerNorm(self.model_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(self.dropout),\n",
    "        #     nn.Linear(self.model_dim, self.num_classes)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "\n",
    "\n",
    "        mask = x.clone().detach()\n",
    "        mask = mask.eq(0)\n",
    "        \n",
    "        x = self.input_net(x)\n",
    "\n",
    "        x = x * (1 - mask.unsqueeze(-1).type_as(x))\n",
    "\n",
    "        x = self.transformer(x, mask=mask)\n",
    "\n",
    "        x = x[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35798921-f736-45e3-811b-0f72e005a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(df):\n",
    "\n",
    "    smiles = df.smiles.values\n",
    "    labels = df.target.values\n",
    "\n",
    "    MAX_LEN = 200\n",
    "    a = featurizer.featurize(smiles)\n",
    "    aa = [i['input_ids'][:MAX_LEN] + (MAX_LEN - len(i['input_ids']) )  * [featurizer.tokenizer.pad_token_id] for i in a]\n",
    "    input_ids = torch.tensor(aa, dtype=torch.int)\n",
    "\n",
    "\n",
    "    return input_ids, labels\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, labels):\n",
    "\n",
    "        self.tokens = tokens\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \n",
    "        sample = {'tokens': self.tokens[idx], 'labels': self.labels[idx] }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d630d34",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066c7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/esol/delaney-processed.csv')\n",
    "df['target'] = df['measured log solubility in mols per litre']\n",
    "train_df, val_df = train_test_split(df, test_size=.2)\n",
    "test_df, val_df = train_test_split(val_df, test_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cebc44",
   "metadata": {},
   "source": [
    "#### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fe979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gihan/.env/p11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_tokenizer = RobertaTokenizerFast.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_60k\")\n",
    "featurizer = HuggingFaceFeaturizer(tokenizer=hf_tokenizer)\n",
    "\n",
    "train_ids, train_labels = get_tokens(train_df)\n",
    "val_ids, val_labels = get_tokens(val_df)\n",
    "test_ids, test_labels = get_tokens(test_df)\n",
    "\n",
    "train_dataset = TextDataset(train_ids, train_labels)\n",
    "val_dataset = TextDataset(val_ids, val_labels)\n",
    "test_dataset = TextDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cba83",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "model = TransformerModel(input_dim=featurizer.tokenizer.vocab_size, model_dim=128, num_heads=2, num_classes=1, num_layers=4)\n",
    "model.to(device);\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=10**-5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "accuracy = MeanSquaredError()\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, device, optimizer):\n",
    "    model.train();\n",
    "    total_loss = total_examples = 0\n",
    "    for data in train_loader:\n",
    "        enc = data['tokens']\n",
    "        y = data['labels']\n",
    "        enc = enc.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(enc)\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) \n",
    "        total_examples += len(data['labels'])\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "\n",
    "def valid_fn(loader, model, device):\n",
    "    accs = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        model.eval();\n",
    "        \n",
    "        for data in loader:\n",
    "            enc = data['tokens']\n",
    "            y = data['labels']\n",
    "            enc = enc.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(enc)\n",
    "            \n",
    "            # predictions = torch.argmax(out, dim=1)\n",
    "            acc = accuracy(out.reshape(-1,), y)\n",
    "\n",
    "            accs+=acc\n",
    "            n+=len(y)\n",
    "\n",
    "    # return float(torch.cat(mse, dim=0).mean().sqrt())\n",
    "    return accs/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e0029",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d395b2a5-3f52-4f4f-8446-3b888905df6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gihan/.env/p11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07029614597558975\n",
      "0.06723970174789429\n",
      "0.06812139600515366\n",
      "0.06751897186040878\n",
      "0.0692100003361702\n",
      "0.06783988326787949\n",
      "0.06701565533876419\n",
      "0.06736886501312256\n",
      "0.07002850621938705\n",
      "0.06947501748800278\n",
      "0.06846916675567627\n",
      "0.06748075038194656\n",
      "0.0676903948187828\n",
      "0.06730744242668152\n",
      "0.06754076480865479\n",
      "0.06798145920038223\n",
      "0.06760700792074203\n",
      "0.0668359026312828\n",
      "0.06742312014102936\n",
      "0.06717287003993988\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    train_fn(train_loader, model, device, optimizer)\n",
    "    mse = valid_fn(val_loader, model, device)\n",
    "\n",
    "    print(mse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdd1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
